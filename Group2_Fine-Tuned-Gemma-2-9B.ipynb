{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada0862e-24ec-444a-9499-ef49825ded48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "HF_TOKEN = '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'\n",
    "# Load the OPUS-100 dataset for English to Hindi\n",
    "dataset = load_dataset(\"opus100\", \"en-hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990dbc4-c4bf-4f1c-8245-1721c16c3c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18f3bff-928e-48f8-b42e-8a25239b7abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1:\n",
      "English: Other, Private Use\n",
      "Hindi: ‡§Ö‡§®‡•ç‡§Ø, ‡§®‡§ø‡§ú‡§º‡•Ä ‡§â‡§™‡§Ø‡•ã‡§ó\n",
      "\n",
      "Row 2:\n",
      "English: [SCREAMING]\n",
      "Hindi: ‡§ä‡§¨‡§°‡§º .\n",
      "\n",
      "Row 3:\n",
      "English: Spouse\n",
      "Hindi: ‡§ú‡•Ä‡§µ‡§®‡§∏‡§æ‡§•‡•Ä\n",
      "\n",
      "Row 4:\n",
      "English: I will never salute you!\n",
      "Hindi: - ‡§§‡•Å‡§Æ ‡§è‡§ï ‡§ï‡§Æ‡§æ‡§Ç‡§°‡§∞ ‡§ï‡§≠‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§ó‡§æ!\n",
      "\n",
      "Row 5:\n",
      "English: and the stars and the trees bow themselves;\n",
      "Hindi: ‡§î‡§∞ ‡§§‡§æ‡§∞‡•á ‡§î‡§∞ ‡§µ‡•É‡§ï‡•ç‡§∑ ‡§∏‡§ú‡§¶‡§æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display 5 rows from the training set\n",
    "for i in range(5):\n",
    "    print(f\"Row {i + 1}:\")\n",
    "    print(f\"English: {dataset['train'][i]['translation']['en']}\")\n",
    "    print(f\"Hindi: {dataset['train'][i]['translation']['hi']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae154709-f2a8-4e24-b0c9-f501764fc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows from OPUS-100:\n",
      "                                       english                           hindi\n",
      "0                           Other, Private Use               ‡§Ö‡§®‡•ç‡§Ø, ‡§®‡§ø‡§ú‡§º‡•Ä ‡§â‡§™‡§Ø‡•ã‡§ó\n",
      "1                                  [SCREAMING]                          ‡§ä‡§¨‡§°‡§º .\n",
      "2                                       Spouse                        ‡§ú‡•Ä‡§µ‡§®‡§∏‡§æ‡§•‡•Ä\n",
      "3                     I will never salute you!  - ‡§§‡•Å‡§Æ ‡§è‡§ï ‡§ï‡§Æ‡§æ‡§Ç‡§°‡§∞ ‡§ï‡§≠‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§ó‡§æ!\n",
      "4  and the stars and the trees bow themselves;  ‡§î‡§∞ ‡§§‡§æ‡§∞‡•á ‡§î‡§∞ ‡§µ‡•É‡§ï‡•ç‡§∑ ‡§∏‡§ú‡§¶‡§æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à;\n"
     ]
    }
   ],
   "source": [
    "# Convert to Pandas DataFrame\n",
    "import pandas as pd\n",
    "opus_df = pd.DataFrame({\n",
    "    \"english\": [entry[\"en\"] for entry in dataset[\"train\"][\"translation\"]],\n",
    "    \"hindi\": [entry[\"hi\"] for entry in dataset[\"train\"][\"translation\"]]\n",
    "})\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"First few rows from OPUS-100:\")\n",
    "print(opus_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1705c5d-176e-4a61-ade8-e16f2845d7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ubuntu/.cache/kagglehub/datasets/vaibhavkumar11/hindi-english-parallel-corpus/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"vaibhavkumar11/hindi-english-parallel-corpus\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9cde0e-7946-4b3f-b57d-286c92370f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the dataset: ['hindi_english_parallel.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the dataset\n",
    "dataset_path = \"/home/ubuntu/.cache/kagglehub/datasets/vaibhavkumar11/hindi-english-parallel-corpus/versions/1\"\n",
    "\n",
    "# List files in the dataset directory\n",
    "files = os.listdir(dataset_path)\n",
    "print(\"Files in the dataset:\", files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a95958-1a6e-40b2-b4d3-fe412660cb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hindi_english_parallel.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f780ab-fbd1-4441-9a63-da6d2b6c1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the dataset file\n",
    "dataset_file = os.path.join(dataset_path, \"hindi_english_parallel.csv\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(dataset_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9970add-5724-494e-95f8-d759c81853a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               hindi  \\\n",
      "0    ‡§Ö‡§™‡§®‡•á ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡•ã ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§µ‡•ç‡§Ø‡§æ‡§Ø‡§æ‡§Æ ‡§ï‡§æ ‡§≤‡§æ‡§≠ ‡§¶‡•á‡§Ç   \n",
      "1                    ‡§è‡§ï‡•ç‡§∏‡•á‡§∞‡•ç‡§∏‡§æ‡§á‡§∏‡§∞ ‡§™‡§π‡•Å‡§Ç‡§ö‡§®‡•Ä‡§Ø‡§§‡§æ ‡§Ö‡§®‡•ç‡§µ‡•á‡§∑‡§ï   \n",
      "2              ‡§®‡§ø‡§ö‡§≤‡•á ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ   \n",
      "3               ‡§ä‡§™‡§∞‡•Ä ‡§™‡§ü‡§≤ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§™‡•ç‡§≤‡§ó-‡§á‡§® ‡§ñ‡§æ‡§ï‡§æ   \n",
      "4  ‡§â‡§® ‡§™‡•ç‡§≤‡§ó-‡§á‡§®‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡•Ç‡§ö‡•Ä ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§°‡§ø‡§´‡•ã‡§≤‡•ç‡§ü ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§®‡§ø...   \n",
      "\n",
      "                                          english  \n",
      "0  Give your application an accessibility workout  \n",
      "1               Accerciser Accessibility Explorer  \n",
      "2  The default plugin layout for the bottom panel  \n",
      "3     The default plugin layout for the top panel  \n",
      "4  A list of plugins that are disabled by default  \n"
     ]
    }
   ],
   "source": [
    "# Inspect the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Access Hindi and English columns\n",
    "hindi_sentences = df['hindi']\n",
    "english_sentences = df['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66586c7-5fd0-4b35-af34-1a7d5a334147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset size: (2096160, 2)\n",
      "First few rows of the combined dataset:\n",
      "                                       english                           hindi\n",
      "0                           Other, Private Use               ‡§Ö‡§®‡•ç‡§Ø, ‡§®‡§ø‡§ú‡§º‡•Ä ‡§â‡§™‡§Ø‡•ã‡§ó\n",
      "1                                  [SCREAMING]                          ‡§ä‡§¨‡§°‡§º .\n",
      "2                                       Spouse                        ‡§ú‡•Ä‡§µ‡§®‡§∏‡§æ‡§•‡•Ä\n",
      "3                     I will never salute you!  - ‡§§‡•Å‡§Æ ‡§è‡§ï ‡§ï‡§Æ‡§æ‡§Ç‡§°‡§∞ ‡§ï‡§≠‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§ó‡§æ!\n",
      "4  and the stars and the trees bow themselves;  ‡§î‡§∞ ‡§§‡§æ‡§∞‡•á ‡§î‡§∞ ‡§µ‡•É‡§ï‡•ç‡§∑ ‡§∏‡§ú‡§¶‡§æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à;\n"
     ]
    }
   ],
   "source": [
    "# Combine the two datasets\n",
    "combined_df = pd.concat([opus_df, df], ignore_index=True)\n",
    "\n",
    "# Display the size of the combined dataset\n",
    "print(f\"Combined dataset size: {combined_df.shape}\")\n",
    "\n",
    "# Display the first few rows of the combined dataset\n",
    "print(\"First few rows of the combined dataset:\")\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9006b5c7-8a25-4376-8c0d-4b2d02c2e275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after removing duplicates: (1635847, 2)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows\n",
    "combined_df = combined_df.drop_duplicates()\n",
    "\n",
    "# Display the size after removing duplicates\n",
    "print(f\"Dataset size after removing duplicates: {combined_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cfdbbfe-7a3d-4de8-8fbe-6584f610cd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with Hindi in English column: 3049\n",
      "                                                  english  \\\n",
      "973263          2. Infection caused by germs. 2. ‡§ú‡•Ä‡§µ‡§æ‡§£‡•Å‡§ì‡§Ç   \n",
      "973343  This position is similar to armchair. In this ...   \n",
      "973383  Many countries in the [unclear], they need leg...   \n",
      "973483  Virtually all groups of plants and animals, an...   \n",
      "973518  Let 's first review what we know does not and ...   \n",
      "\n",
      "                                                    hindi  hindi_in_english  \n",
      "973263                             ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§∏‡§Ç‡§ï‡•ç‡§∞‡§æ‡§Æ‡§£ ‡§π‡•ã‡§®‡§æ‡•§               True  \n",
      "973343  ‡§Ø‡§π ‡§π‡§§‡•ç‡§•‡§æ‡§ï‡•Å‡§∞‡•ç‡§∏‡•Ä ‡§∏‡•á ‡§Æ‡§ø‡§≤‡§§‡•Ä ‡§ú‡•Å‡§≤‡§§‡•Ä ‡§™‡•ã‡§ú‡•Ä‡§∂‡§® ‡§π‡•à ‡§á‡§∏‡§Æ‡•á‡§Ç ...              True  \n",
      "973383          ‡§ï‡•á ‡§¨‡§π‡•Å‡§§ ‡§∏‡§æ‡§∞‡•á ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•ã‡§Ç ‡§ï‡•ã ‡§Æ‡§æ‡§®‡•ç‡§Ø‡§§‡§æ ‡§ö‡§æ‡§π‡§ø‡§è.               True  \n",
      "973483  ‡§™‡•å‡§ß‡•ã‡§Ç ‡§§‡§•‡§æ ‡§ú‡§æ‡§®‡§µ‡§∞‡•ã‡§Ç ‡§ï‡•á ‡§∏‡§≠‡•Ä ‡§∏‡§Æ‡•Ç‡§π‡•ã‡§Ç ‡§î‡§∞ ‡§â‡§®‡§ï‡•á ‡§Ö‡§®‡•ç‡§¶‡§∞ ...              True  \n",
      "973518                                  ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ‚Äù ‡§ï‡§æ ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§®              True  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to check if a string contains Hindi characters\n",
    "def contains_hindi(text):\n",
    "    if not isinstance(text, str):  # Ensure the input is a string\n",
    "        return False\n",
    "    hindi_pattern = re.compile('[\\u0900-\\u097F]')\n",
    "    return bool(hindi_pattern.search(text))\n",
    "\n",
    "# Apply the function to the English column\n",
    "combined_df['hindi_in_english'] = combined_df['english'].apply(contains_hindi)\n",
    "\n",
    "# Display rows where Hindi words are found in the English column\n",
    "hindi_in_english_rows = combined_df[combined_df['hindi_in_english']]\n",
    "print(f\"Number of rows with Hindi in English column: {len(hindi_in_english_rows)}\")\n",
    "print(hindi_in_english_rows.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23f1028-aec2-4115-a7e6-280d4f58faa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with English in Hindi column: 116543\n",
      "                                      english  \\\n",
      "5        _Download Messages for Offline Usage   \n",
      "8   The application '%s' could not be created   \n",
      "17                                  Kennebunk   \n",
      "20                          FIB(9) returns 34   \n",
      "25                                Third power   \n",
      "\n",
      "                                                hindi  hindi_in_english  \\\n",
      "5       ‡§ë‡§´‡§º‡§≤‡§æ‡§á‡§® ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è ‡§∏‡§Ç‡§¶‡•á‡§∂ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç (_D)             False   \n",
      "8   ‡§Ö‡§®‡•Å‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó '%s' ‡§ï‡•á ‡§≤‡§ø‡§è ‡§á‡§∏‡•ç‡§§‡•á‡§Æ‡§æ‡§≤ ‡§ï‡§ø‡§Ø‡§æ ‡§ú‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ...             False   \n",
      "17  ‡§ï‡•á‡§®‡•á‡§¨‡§Ç‡§ïCity name (optional, probably does not ...             False   \n",
      "20                          FIB( 9) ‡§ï‡§æ ‡§™‡§∞‡§ø‡§£‡§æ‡§Æ ‡§π‡•ã‡§ó‡§æ 34             False   \n",
      "25                       ‡§§‡•É‡§§‡•Ä‡§Ø ‡§ò‡§æ‡§§x to the power of y             False   \n",
      "\n",
      "    english_in_hindi  \n",
      "5               True  \n",
      "8               True  \n",
      "17              True  \n",
      "20              True  \n",
      "25              True  \n"
     ]
    }
   ],
   "source": [
    "# Function to check if a string contains English characters\n",
    "def contains_english(text):\n",
    "    if not isinstance(text, str):  # Ensure the input is a string\n",
    "        return False\n",
    "    english_pattern = re.compile('[A-Za-z]')\n",
    "    return bool(english_pattern.search(text))\n",
    "\n",
    "# Apply the function to the Hindi column\n",
    "combined_df['english_in_hindi'] = combined_df['hindi'].apply(contains_english)\n",
    "\n",
    "# Display rows where English words are found in the Hindi column\n",
    "english_in_hindi_rows = combined_df[combined_df['english_in_hindi']]\n",
    "print(f\"Number of rows with English in Hindi column: {len(english_in_hindi_rows)}\")\n",
    "print(english_in_hindi_rows.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73e92239-521f-43db-9fb3-3b7aaff64d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after removing unwanted columns: 1635847\n",
      "First few rows of the cleaned dataset:\n",
      "                                       english                           hindi\n",
      "0                           Other, Private Use               ‡§Ö‡§®‡•ç‡§Ø, ‡§®‡§ø‡§ú‡§º‡•Ä ‡§â‡§™‡§Ø‡•ã‡§ó\n",
      "1                                  [SCREAMING]                          ‡§ä‡§¨‡§°‡§º .\n",
      "2                                       Spouse                        ‡§ú‡•Ä‡§µ‡§®‡§∏‡§æ‡§•‡•Ä\n",
      "3                     I will never salute you!  - ‡§§‡•Å‡§Æ ‡§è‡§ï ‡§ï‡§Æ‡§æ‡§Ç‡§°‡§∞ ‡§ï‡§≠‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•ã‡§ó‡§æ!\n",
      "4  and the stars and the trees bow themselves;  ‡§î‡§∞ ‡§§‡§æ‡§∞‡•á ‡§î‡§∞ ‡§µ‡•É‡§ï‡•ç‡§∑ ‡§∏‡§ú‡§¶‡§æ ‡§ï‡§∞‡§§‡•á ‡§π‡•à;\n"
     ]
    }
   ],
   "source": [
    "# Drop the columns 'hindi_in_english' and 'english_in_hindi' from the DataFrame\n",
    "cleaned_df = combined_df.drop(columns=['hindi_in_english', 'english_in_hindi'])\n",
    "\n",
    "# Display the size of the cleaned dataset\n",
    "print(f\"Dataset size after removing unwanted columns: {len(cleaned_df)}\")\n",
    "\n",
    "# Inspect the first few rows of the cleaned dataset\n",
    "print(\"First few rows of the cleaned dataset:\")\n",
    "print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b7f08d19-91bd-4fc0-9c39-5197f3b5ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9B LR - 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd677379-29b9-42a5-a6ae-a9f6977723ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Randomly sample 10,000 rows from cleaned_df\n",
    "df = cleaned_df.sample(n=10000, random_state=42)\n",
    "\n",
    "# Rename columns for consistency\n",
    "df = df.rename(columns={'hindi': 'source', 'english': 'target'})\n",
    "\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Split dataset into train, validation, and test\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "remaining_df = df.drop(train_df.index)\n",
    "valid_df = remaining_df.sample(frac=0.5, random_state=42)\n",
    "test_df = remaining_df.drop(valid_df.index)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "089b4c93-f903-4188-9875-ed73bb2ebb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9974 entries, 1293567 to 2089657\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  9974 non-null   object\n",
      " 1   source  9974 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 491.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3de400eb-e5dc-4a87-b662-a740bdc1afe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7979/7979 [00:00<00:00, 8354.74 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 998/998 [00:00<00:00, 10162.21 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 997/997 [00:00<00:00, 10609.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "access_token= \"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\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\",token=access_token)\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples['source'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        examples['target'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Preprocess datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set dataset format for PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "valid_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15b84707-1354-4d9e-8c49-dee1846d86f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "2024-12-09 08:40:44.295101: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-09 08:40:44.315315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-09 08:40:44.340767: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-09 08:40:44.347988: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-09 08:40:44.362862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [07:21<00:00, 110.26s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:06<00:00,  1.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "access_token = \"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\"  # Replace with your actual Hugging Face token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-9b-it\",         # Correct model name\n",
    "    device_map=\"auto\",            # Automatically map to available devices\n",
    "    torch_dtype=torch.float16,   # Use bfloat16 for efficiency\n",
    "    use_auth_token=access_token   # Pass your access token for authentication\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ac5341c-7b97-4496-9d12-b8c8413ad266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4933cf3-fd6b-41c3-9733-9e3ec3a1bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LoRA configuration\n",
    "# lora_config = LoraConfig(\n",
    "#     task_type=TaskType.CAUSAL_LM,  # Task type for causal language modeling\n",
    "#     inference_mode=False,         # Fine-tune mode\n",
    "#     r=8,                          # Low-rank dimension\n",
    "#     lora_alpha=32,                # Scaling factor\n",
    "#     lora_dropout=0.1,             # Dropout for LoRA layers\n",
    "# )\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    #target_modules=modules,\n",
    "    lora_dropout=0.02,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7aa70a16-7f87-4291-bd10-4af91c01f53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=499,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=499,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    save_safetensors=False  # Use PyTorch save format\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd7aec1b-b322-4451-9e5a-877a16cb907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4990' max='4990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4990/4990 42:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>2.856400</td>\n",
       "      <td>2.670529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>2.415700</td>\n",
       "      <td>2.540451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1497</td>\n",
       "      <td>2.215400</td>\n",
       "      <td>2.559234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1996</td>\n",
       "      <td>2.026400</td>\n",
       "      <td>2.624841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2495</td>\n",
       "      <td>1.837000</td>\n",
       "      <td>2.706093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2994</td>\n",
       "      <td>1.666400</td>\n",
       "      <td>2.864666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3493</td>\n",
       "      <td>1.508600</td>\n",
       "      <td>3.027435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3992</td>\n",
       "      <td>1.361000</td>\n",
       "      <td>3.206076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4491</td>\n",
       "      <td>1.243600</td>\n",
       "      <td>3.403177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4990</td>\n",
       "      <td>1.153700</td>\n",
       "      <td>3.544007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' argument of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'max_batch_size' argument instead.\n",
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "model.tie_weights = lambda: None  # Temporarily disable shared weights\n",
    "trainer.train()                   # Proceed with training\n",
    "model.tie_weights()               # Re-enable shared weights after training\n",
    "\n",
    "# Save the model after training\n",
    "trainer.save_model(\"./results\")  # Explicitly save the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8252a0e7-52b8-4cf9-86e9-ca3728f7f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2101969b-8818-42c2-899f-22062c84bb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for BLEU evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:34<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: {'bleu': 0.8884162322663213, 'precisions': [0.9571984435797666, 0.9182692307692307, 0.8742138364779874, 0.8363636363636363], 'brevity_penalty': 0.9922481009857891, 'length_ratio': 0.9922779922779923, 'translation_length': 257, 'reference_length': 259}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the BLEU metric\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "# Load the test data from the CSV file\n",
    "test_data_path = \"test_file.csv\"  # Path to your test CSV file\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# Ensure the columns match expectations\n",
    "test_df = test_df.rename(columns={\"Source\": \"source\", \"Target\": \"target\"})\n",
    "\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "raw_test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Function to generate predictions\n",
    "def generate_translation(model, tokenizer, text, max_length=256):\n",
    "    prompt = f\"Translate the following Hindi text to English:\\n\\n{text}\\n\\nEnglish Translation:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_length).to(\"cuda\")\n",
    "    outputs = model.generate(inputs.input_ids, max_new_tokens=100, num_beams=5, early_stopping=True)\n",
    "    translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translation.split(\"English Translation:\")[-1].strip()  # Remove prompt if included\n",
    "\n",
    "# Generate predictions and references\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(\"Generating predictions for BLEU evaluation...\")\n",
    "for example in tqdm(raw_test_dataset):  # Iterate over the test dataset\n",
    "    source_text = example[\"source\"]  # Raw Hindi text\n",
    "    reference_text = example[\"target\"]  # Reference English translation\n",
    "    prediction = generate_translation(model, tokenizer, source_text)\n",
    "    predictions.append(prediction)\n",
    "    references.append([reference_text])  # Wrap the reference in a list\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"\\nBLEU Score:\", bleu_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
